---
widget: blank

# Activate this widget? true/false
active: true

# This file represents a page section.
headless: true

# Order that this section appears on the page.
weight: 50

title: News
subtitle:

design:
  columns: "2"
  background:
#    image: open-book.jpg
#    image_darken: 1.0
#    image_parallax: true
#    image_position: center
#    image_size: cover
#    text_color_light: false
  spacing:
    padding: ["50px", "100px", "50px", "100px"]
---

* 2025.9: üöÄ We release [GenExam](https://arxiv.org/abs/2509.14232), a benchmark for evaluating text-to-image generation in exam-style settings across multiple disciplines.

* 2025.9 üèÜ Our paper [Limit of RLVR](https://arxiv.org/abs/2504.13837) on reinforcement learning for LLM is accepted by **NeurIPS 2025** as **Oral (Top 0.3%)** and awarded the **Best Paper Award (2/172)** of **ICML AI4MATH Workshop 2025!**

* 2025.8: üöÄ We release [InternVL3.5](https://arxiv.org/abs/2508.18265), a leading multimodal large language model with advanced versatility, reasoning, and efficiency.

* 2025.8: üèÜ Our paper [Sparkle](https://arxiv.org/abs/2410.16162) on VLM spatial reasoning is accepted by **EMNLP 2025 Findings** and awarded the **Best Paper Award** at **IJCAI MKLM Workshop 2025**.

* 2025.7: ‚≠êÔ∏è Our paper [PIIP](https://arxiv.org/abs/2501.07783) on efficient multimodal understanding is accepted by **TPAMI**.

* 2025.6: ‚≠êÔ∏è Our paper [V2M Survey](https://arxiv.org/abs/2503.21254) on vision-to-music generation is accepted by **ISMIR 2025**.

* 2025.2: ‚≠êÔ∏è Our papers [Mono-InternVL](https://arxiv.org/abs/2410.08202) and [SynerGen-VL](https://arxiv.org/abs/2412.09604) on encoder-free MLLMs are accepted by **CVPR 2025**.

  
