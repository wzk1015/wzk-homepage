---
widget: blank

# Activate this widget? true/false
active: true

# This file represents a page section.
headless: false

# Order that this section appears on the page.
weight: 100

title: Publications
subtitle:

design:
  columns: "1"
  background:
    spacing:
      padding: ["50px", "100px", "50px", "100px"]

---

<br>
<br>

<div class="publication-list">
<div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex:1; max-width: 400px;">
      <img src="pub_imgs/genexam.png" alt="genexam thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex:3;  margin-left: 20px;">
      <p><strong>GenExam: A Multidisciplinary Text-to-Image Exam</strong></p>
      <p><em><u>Zhaokai Wang*</u>, Penghao Yin*, Xiangyu Zhao, Changyao Tian, Yu Qiao, Wenhai Wang, Jifeng Dai, Gen Luo</em></p>
      <p>Preprint</p>
      <p><a href="https://arxiv.org/abs/2509.14232">[Paper]</a> <a href="https://github.com/OpenGVLab/GenExam">[Code]</a> <a href="https://huggingface.co/datasets/OpenGVLab/GenExam">[Dataset]</a> <a href="https://mp.weixin.qq.com/s/r0J8AwQnC-_66PW3vqZKBA">[Post]</a></p>
    </div>
  </div>




<div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex:1; max-width: 400px;">
      <img src="pub_imgs/mono_v1.5.jpg" alt="mono_v1.5 thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex:3;  margin-left: 20px;">
      <p><strong>Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large Language Models</strong></p>
      <p><em>Gen Luo, Wenhan Dou, Wenhao Li, <u>Zhaokai Wang</u>, Xue Yang, Changyao Tian, Hao Li, Weiyun Wang, Wenhai Wang, Xizhou Zhu, Yu Qiao, Jifeng Dai</em></p>
      <p>Preprint</p>
      <p><a href="https://arxiv.org/abs/2507.12566">[Paper]</a> <a href="https://github.com/OpenGVLab/mono-internvl">[Code]</a></p>
    </div>
  </div>

<div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex:1; max-width: 400px;">
      <img src="pub_imgs/internvl3.5.png" alt="mono_v1.5 thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex:3;  margin-left: 20px;">
      <p><strong>InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency</strong></p>
      <p><em>Weiyun Wang*, Zhangwei Gao*, Lixin Gu*, Hengjun Pu*, Long Cui*, Xingguang Wei*, Zhaoyang Liu*, Linglin Jing*, Shenglong Ye*, Jie Shao*, <u>Zhaokai Wang*</u>, Zhe Chen*, Hongjie Zhang, Ganlin Yang, Haomin Wang, Qi Wei, Jinhui Yin, Wenhao Li, Erfei Cui, Guanzhou Chen, Zichen Ding, Changyao Tian, Zhenyu Wu, Jingjing Xie, Zehao Li, Bowen Yang, Yuchen Duan, Xuehui Wang, Zhi Hou, Haoran Hao, Tianyi Zhang, Songze Li, Xiangyu Zhao, Haodong Duan, Nianchen Deng, Bin Fu, Yinan He, Yi Wang, Conghui He, Botian Shi, Junjun He, Yingtong Xiong, Han Lv, Lijun Wu, Wenqi Shao, Kaipeng Zhang, Huipeng Deng, Biqing Qi, Jiaye Ge, Qipeng Guo, Wenwei Zhang, Songyang Zhang, Maosong Cao, Junyao Lin, Kexian Tang, Jianfei Gao, Haian Huang, Yuzhe Gu, Chengqi Lyu, Huanze Tang, Rui Wang, Haijun Lv, Wanli Ouyang, Limin Wang, Min Dou, Xizhou Zhu, Tong Lu, Dahua Lin, Jifeng Dai, Weijie Su, Bowen Zhou, Kai Chen, Yu Qiao, Wenhai Wang, Gen Luo</em></p>
      <p>Technical Report</p>
      <p><a href="https://arxiv.org/abs/2508.18265">[Paper]</a> <a href="https://github.com/OpenGVLab/InternVL">[Code]</a> <a href="https://chat.intern-ai.org.cn/">[Demo]</a> </p>
    </div>
</div>


 <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex:1; max-width: 400px;">
      <img src="pub_imgs/vmb.png" alt="VMB thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex:3;  margin-left: 20px;">
      <p><strong>Multimodal Music Generation with Explicit Bridges and Retrieval Augmentation</strong></p>
      <p><em>Baisen Wang, Le Zhuo, <u>Zhaokai Wang</u>, Chenxi Bao, Chengjing Wu, Xuecheng Nie, Jiao Dai, Jizhong Han, Yue Liao, Si Liu</em></p>
      <p><strong>ISMIR 2025 LLM4Music Workshop</strong></p>
      <p><a href="https://arxiv.org/abs/2412.09428">[Paper]</a> <a href="https://github.com/wbs2788/VMB">[Code]</a> <a href="https://wzk1015.github.io/vmb/">[Demo]</a></p>
    </div>
  </div>

<div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex:1; max-width: 400px;">
      <img src="pub_imgs/piip_v2.jpg" alt="PIIP_v2 thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex:3;  margin-left: 20px;">
      <p><strong>Parameter-Inverted Image Pyramid Networks for Visual Perception and Multimodal Understanding</strong></p>
      <p><em><u>Zhaokai Wang*</u>, Xizhou Zhu*, Xue Yang*, Gen Luo, Hao Li, Changyao Tian, Wenhan Dou, Junqi Ge, Lewei Lu, Yu Qiao, Jifeng Dai</em></p>
      <p><strong>TPAMI 2025</strong></p>
      <p><a href="https://arxiv.org/abs/2501.07783">[Paper]</a> <a href="https://github.com/OpenGVLab/PIIP">[Code]</a></p>
    </div>
  </div>
<div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex:1; max-width: 400px;">
      <img src="pub_imgs/rlvr.jpg" alt="RLVR thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex:3;  margin-left: 20px;">
      <p><strong>Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?</strong></p>
      <p><em>Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, <u>Zhaokai Wang</u>, Yang Yue, Shiji Song, Gao Huang</em></p>
      <p><strong>NeurIPS 2025 <span style="color: #ff0000;">Best Paper Runner Up <a href="https://papercopilot.com/statistics/neurips-statistics/neurips-2025-statistics/">(Top 1 score among 21575 submissions)</a></span> &amp; ICML 2025 AI4MATH Workshop <span style="color: #ff0000;">Best Paper Award (2/172)</span></strong></p>
      <p><a href="https://arxiv.org/abs/2504.13837">[Paper]</a> <a href="https://limit-of-rlvr.github.io/">[Project Page]</a> <a href="https://github.com/LeapLabTHU/limit-of-RLVR">[Code]</a></p>
    </div>
 </div>



  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex:1; max-width: 400px;">
      <img src="pub_imgs/sparkle.jpg" alt="Sparkle thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex:3;  margin-left: 20px;">
      <p><strong>Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models Elicits Generalization to Composite Spatial Reasoning</strong></p>
      <p><em>Yihong Tang*, Ao Qu*, <u>Zhaokai Wang*</u>, Dingyi Zhuang*, Zhaofeng Wu, Wei Ma, Shenhao Wang, Yunhan Zheng, Zhan Zhao, Jinhua Zhao</em></p>
      <p><strong>EMNLP 2025 Findings &amp; IJCAI 2025 MKLM Workshop <span style="color: #ff0000;">Best Paper Award</span></strong></p>
      <p><a href="https://arxiv.org/abs/2410.16162">[Paper]</a></p>
    </div>
  </div>



  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex:1; max-width: 400px;">
      <img src="pub_imgs/v2m-survey.jpg" alt="V2M survey thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex:3;  margin-left: 20px;">
      <p><strong>Vision-to-Music Generation: A Survey</strong></p>
      <p><em><u>Zhaokai Wang</u>, Chenxi Bao, Le Zhuo, Jingrui Han, Yang Yue, Yihong Tang, Victor Shea-Jay Huang, Yue Liao</em></p>
      <p><strong>ISMIR 2025</strong></p>
      <p><a href="https://arxiv.org/abs/2503.21254">[Paper]</a> <a href="https://github.com/wzk1015/Awesome-Vision-to-Music-Generation">[Repo]</a> <a href="https://www.youtube.com/watch?v=JwV05mMlOG0">[Video]</a></p>
    </div>
  </div>



  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex:1; max-width: 400px;">
      <img src="pub_imgs/mono-internvl.png" alt="Mono-InternVL thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex:3;  margin-left: 20px;">
      <p><strong>Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training</strong></p>
      <p><em>Gen Luo*, Xue Yang*, Wenhan Dou*, <u>Zhaokai Wang*</u>, Jiawen Liu, Jifeng Dai, Yu Qiao, Xizhou Zhu</em></p>
      <p><strong>CVPR 2025</strong></p>
      <p><a href="https://arxiv.org/abs/2410.08202">[Paper]</a> <a href="https://internvl.github.io/blog/2024-10-10-Mono-InternVL/">[Project Page]</a> <a href="https://github.com/OpenGVLab/Mono-InternVL">[Code]</a> <a href="https://huggingface.co/collections/OpenGVLab/mono-internvl-6707cb402afb22f1e29f4d2b">[Model]</a> <a href="https://mp.weixin.qq.com/s/FmjG0Gp5ow7mm2Vzd9ppPg">[Post]</a> <a href="https://www.wzk.plus/slides/Mono-InternVL_talk.pdf)">[Slides]</a></p>
    </div>
  </div>

  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex:1; max-width: 400px;">
      <img src="pub_imgs/synergen.jpg" alt="SynerGen-VL thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex:3;  margin-left: 20px;">
      <p><strong>SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding</strong></p>
      <p><em>Hao Li, Changyao Tian, Jie Shao, Xizhou Zhu, <u>Zhaokai Wang</u>, Jinguo Zhu, Wenhan Dou, Xiaogang Wang, Hongsheng Li, Lewei Lu, Jifeng Dai</em></p>
      <p><strong>CVPR 2025</strong></p>
      <p><a href="https://arxiv.org/abs/2412.09604">[Paper]</a></p>
    </div>
  </div>
  

  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex:1; max-width: 400px;">
      <img src="pub_imgs/piip.png" alt="PIIP thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex:3;  margin-left: 20px;">
      <p><strong>Parameter-Inverted Image Pyramid Networks</strong></p>
      <p><em>Xizhou Zhu*, Xue Yang*, <u>Zhaokai Wang*</u>, Hao Li, Wenhan Dou, Junqi Ge, Lewei Lu, Yu Qiao, Jifeng Dai</em></p>
      <p><strong>NeurIPS 2024 <span style="color: #ff0000;">Spotlight</span> <a href="https://papercopilot.com/statistics/neurips-statistics/neurips-2024-statistics/" style="color: #0000ff;">- Ranked Top 10 in NeurIPS 2024 (among 15671 submissions), Top 2 in Computer Vision Area</a></strong></p>
      <p><a href="https://arxiv.org/abs/2406.04330">[Paper]</a> <a href="https://github.com/OpenGVLab/PIIP">[Code]</a> <a href="https://zhuanlan.zhihu.com/p/705734540">[Post]</a> <a href="https://www.wzk.plus/slides/PIIP_slides.pdf">[Slides]</a> <a href="https://youtu.be/Kdh3CNp8bfg">[Video]</a></p>
    </div>
  </div>


  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex:1; max-width: 400px;">
      <img src="pub_imgs/itinera2.jpg" alt="ITINERA thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex:3;  margin-left: 20px;">
      <p><strong>ITINERA: Integrating Spatial Optimization with Large Language Models for Open-domain Urban Itinerary Planning</strong></p>
      <p><em>Yihong Tang*, <u>Zhaokai Wang*</u>, Ao Qu*, Yihao Yan*, Zhaofeng Wu, Dingyi Zhuang, Jushi Kai, Kebing Hou, Xiaotong Guo, Han Zheng, Tiange Luo, Jinhua Zhao, Zhan Zhao, Wei Ma</em></p>
      <p><strong>EMNLP 2024 Industry Track &amp; KDD 2024 UrbComp Workshop <span style="color: #ff0000;">Best Paper Award</span></strong></p>
      <p><a href="https://arxiv.org/abs/2402.07204">[Paper]</a> <a href="https://github.com/YihongT/ITINERA">[Code]</a> <a href="https://mp.weixin.qq.com/s/44mtENyqrHiNEEcWS61COg">[Post]</a> <a href="https://s3.amazonaws.com/pf-user-files-01/u-59356/uploads/2024-10-27/lk23u3q/PRE_EMNLP_ITINERA.pdf">[Slides]</a> <a href="https://s3.amazonaws.com/pf-user-files-01/u-59356/uploads/2024-10-27/iw03u0v/PRE_video.mp4">[Video]</a></p>
    </div>
  </div>





  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex:1; max-width: 400px;">
      <img src="pub_imgs/auto_mc_reward.png" alt="Auto MC-Reward thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex:3;  margin-left: 20px;">
      <p><strong>Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft</strong></p>
      <p><em>Hao Li*, Xue Yang*, <u>Zhaokai Wang*</u>, Xizhou Zhu, Jie Zhou, Yu Qiao, Xiaogang Wang, Hongsheng Li, Lewei Lu, Jifeng Dai</em></p>
      <p><strong>CVPR 2024</strong></p>
      <p><a href="https://arxiv.org/abs/2312.09238">[Paper]</a> <a href="https://yangxue0827.github.io/auto_mc-reward.html">[Project Page]</a> <a href="https://mp.weixin.qq.com/s/P2yCkUKnqYFJiY9bDtppLQ">[Post]</a></p>
    </div>
  </div>


  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex:1; max-width: 400px;">
      <img src="pub_imgs/musprod.png" alt="Musprod thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex:3;  margin-left: 20px;">
      <p><strong>Video Background Music Generation: Dataset, Method and Evaluation</strong></p>
      <p><em>Le Zhuo*, <u>Zhaokai Wang*</u>, Baisen Wang*, Yue Liao, Chenxi Bao, Stanley Peng, Songhao Han, Aixi Zhang, Fei Fang, Si Liu</em></p>
      <p><strong>ICCV 2023</strong></p>
      <p><a href="https://arxiv.org/abs/2211.11248">[Paper]</a> <a href="https://drive.google.com/drive/folders/1ASY44xqWGZgKkcHhpzWlOhIbUIMe_epQ?usp=sharing">[Demo]</a></p>
    </div>
  </div>



  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex:1; max-width: 400px;">
      <img src="pub_imgs/cmt.png" alt="CMT thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex:3;  margin-left: 20px;">
      <p><strong>Video Background Music Generation with Controllable Music Transformer</strong></p>
      <p><em>Shangzhe Di, Zeren Jiang, Si Liu, <u>Zhaokai Wang</u>, Leyan Zhu, Zexin He, Hongming Liu, Shuicheng Yan</em></p>
      <p><strong>ACM MM 2021 <span style="color: #ff0000;">Best Paper Award (1/1942)</span></strong></p>
      <p><a href="https://arxiv.org/abs/2111.08380">[Paper]</a> <a href="https://wzk1015.github.io/cmt/">[Project Page]</a> <a href="https://github.com/wzk1015/video-bgm-generation">[Code]</a> <a href="https://colab.research.google.com/github/wzk1015/video-bgm-generation/blob/main/CMT.ipynb">[Demo]</a> <a href="https://mp.weixin.qq.com/s/2aFgIq4-zA9tlgGSNuxzWg">[Post]</a> <a href="https://news.buaa.edu.cn/info/1005/54971.htm">[News]</a></p>
    </div>
  </div>



  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex:1; max-width: 400px;">
      <img src="pub_imgs/cnmt.png" alt="CNMT thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex:3;  margin-left: 20px;">
      <p><strong>Confidence-aware Non-repetitive Multimodal Transformers for TextCaps</strong></p>
      <p><em><u>Zhaokai Wang</u>, Renda Bao, Qi Wu, Si Liu</em></p>
      <p><strong>AAAI 2021</strong></p>
      <p><a href="https://arxiv.org/abs/2012.03662">[Paper]</a> <a href="https://github.com/wzk1015/CNMT">[Code]</a></p>
    </div>
  </div>
