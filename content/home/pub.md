---
widget: blank

# Activate this widget? true/false
active: true

# This file represents a page section.
headless: false

# Order that this section appears on the page.
weight: 100

title: Selected Publications
subtitle:

design:
  columns: "1"
  background:
    spacing:
      padding: ["50px", "100px", "50px", "100px"]

---

<br>
<br>

<div class="publication-list">

<div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex: 2; max-width: 800px;">
      <img src="pub_imgs/piip_v2.jpg" alt="PIIP_v2 thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex: 4; margin-left: 20px;">
      <p><strong>Parameter-Inverted Image Pyramid Networks for Visual Perception and Multimodal Understanding</strong></p>
      <p><em><strong>Zhaokai Wang</strong>, Xizhou Zhu, Xue Yang, Gen Luo, Hao Li, Changyao Tian, Wenhan Dou, Junqi Ge, Lewei Lu, Yu Qiao, Jifeng Dai</em></p>
      <p>Preprint</p>
      <p><a href="https://arxiv.org/abs/2501.07783">[Paper]</a> <a href="https://github.com/OpenGVLab/PIIP">[Code]</a></p>
    </div>
  </div>
  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex: 2; max-width: 800px;">
      <img src="pub_imgs/v2m-survey.jpg" alt="V2M survey thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex: 4; margin-left: 20px;">
      <p><strong>Vision-to-Music Generation: A Survey</strong></p>
      <p><em><strong>Zhaokai Wang</strong>, Chenxi Bao, Le Zhuo, Jingrui Han, Yang Yue, Yihong Tang, Victor Shea-Jay Huang, Yue Liao</em></p>
      <p>Preprint</p>
      <p><a href="https://arxiv.org/abs/2503.21254">[Paper]</a> <a href="https://github.com/wzk1015/Awesome-Vision-to-Music-Generation">[Repo]</a></p>
    </div>
  </div>


  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex: 2; max-width: 800px;">
      <img src="pub_imgs/sparkle.jpg" alt="Sparkle thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex: 4; margin-left: 20px;">
      <p><strong>Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models Elicits Generalization to Composite Spatial Reasoning</strong></p>
      <p><em>Yihong Tang*, Ao Qu*, <strong>Zhaokai Wang*</strong>, Dingyi Zhuang*, Zhaofeng Wu, Wei Ma, Shenhao Wang, Yunhan Zheng, Zhan Zhao, Jinhua Zhao</em></p>
      <p>Preprint</p>
      <p><a href="https://arxiv.org/abs/2410.16162">[Paper]</a></p>
    </div>
  </div>
  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex: 2; max-width: 800px;">
      <img src="pub_imgs/vmb.png" alt="VMB thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex: 4; margin-left: 20px;">
      <p><strong>Multimodal Music Generation with Explicit Bridges and Retrieval Augmentation</strong></p>
      <p><em>Baisen Wang, Le Zhuo, <strong>Zhaokai Wang</strong>, Chenxi Bao, Chengjing Wu, Xuecheng Nie, Jiao Dai, Jizhong Han, Yue Liao, Si Liu</em></p>
      <p>Preprint</p>
      <p><a href="https://arxiv.org/abs/2412.09428">[Paper]</a> <a href="https://github.com/wbs2788/VMB">[Code]</a> <a href="https://wzk1015.github.io/vmb/">[Demo]</a></p>
    </div>
  </div>


  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex: 2; max-width: 800px;">
      <img src="pub_imgs/mono-internvl.png" alt="Mono-InternVL thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex: 4; margin-left: 20px;">
      <p><strong>Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training</strong></p>
      <p><em>Gen Luo*, Xue Yang*, Wenhan Dou*, <strong>Zhaokai Wang*</strong>, Jiawen Liu, Jifeng Dai, Yu Qiao, Xizhou Zhu</em></p>
      <p>CVPR 2025</p>
      <p><a href="https://arxiv.org/abs/2410.08202">[Paper]</a> <a href="https://internvl.github.io/blog/2024-10-10-Mono-InternVL/">[Project Page]</a> <a href="https://github.com/OpenGVLab/Mono-InternVL">[Code]</a> <a href="https://huggingface.co/collections/OpenGVLab/mono-internvl-6707cb402afb22f1e29f4d2b">[Model]</a> <a href="https://mp.weixin.qq.com/s/FmjG0Gp5ow7mm2Vzd9ppPg">[Post]</a></p>
    </div>
  </div>

  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex: 2; max-width: 800px;">
      <img src="pub_imgs/synergen.jpg" alt="SynerGen-VL thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex: 4; margin-left: 20px;">
      <p><strong>SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding</strong></p>
      <p><em>Hao Li, Changyao Tian, Jie Shao, Xizhou Zhu, <strong>Zhaokai Wang</strong>, Jinguo Zhu, Wenhan Dou, Xiaogang Wang, Hongsheng Li, Lewei Lu, Jifeng Dai</em></p>
      <p>CVPR 2025</p>
      <p><a href="https://arxiv.org/abs/2412.09604">[Paper]</a></p>
    </div>
  </div>

  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex: 2; max-width: 800px;">
      <img src="pub_imgs/piip.png" alt="PIIP thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex: 4; margin-left: 20px;">
      <p><strong>Parameter-Inverted Image Pyramid Networks</strong></p>
      <p><em>Xizhou Zhu*, Xue Yang*, <strong>Zhaokai Wang*</strong>, Hao Li, Wenhan Dou, Junqi Ge, Lewei Lu, Yu Qiao, Jifeng Dai</em></p>
      <p>NeurIPS 2024 <strong>Spotlight (Top 2.5%)</strong></p>
      <p><a href="https://arxiv.org/abs/2406.04330">[Paper]</a> <a href="https://github.com/OpenGVLab/PIIP">[Code]</a> <a href="https://zhuanlan.zhihu.com/p/705734540">[Post]</a> <a href="https://www.wzk.plus/slides/PIIP_slides.pdf">[Slides]</a> <a href="https://youtu.be/Kdh3CNp8bfg">[Video]</a></p>
    </div>
  </div>


  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex: 2; max-width: 800px;">
      <img src="pub_imgs/itinera2.jpg" alt="ITINERA thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex: 4; margin-left: 20px;">
      <p><strong>ITINERA: Integrating Spatial Optimization with Large Language Models for Open-domain Urban Itinerary Planning</strong></p>
      <p><em>Yihong Tang*, <strong>Zhaokai Wang*</strong>, Ao Qu*, Yihao Yan*, Zhaofeng Wu, Dingyi Zhuang, Jushi Kai, Kebing Hou, Xiaotong Guo, Han Zheng, Tiange Luo, Jinhua Zhao, Zhan Zhao, Wei Ma</em></p>
      <p>KDD UrbComp 2024 <strong>Best Paper Award</strong> & EMNLP 2024</p>
      <p><a href="https://arxiv.org/abs/2402.07204">[Paper]</a> <a href="https://github.com/YihongT/ITINERA">[Code]</a> <a href="https://mp.weixin.qq.com/s/44mtENyqrHiNEEcWS61COg">[Post]</a> <a href="https://s3.amazonaws.com/pf-user-files-01/u-59356/uploads/2024-10-27/lk23u3q/PRE_EMNLP_ITINERA.pdf">[Slides]</a> <a href="https://s3.amazonaws.com/pf-user-files-01/u-59356/uploads/2024-10-27/iw03u0v/PRE_video.mp4">[Video]</a></p>
    </div>
  </div>




  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex: 2; max-width: 800px;">
      <img src="pub_imgs/auto_mc_reward.png" alt="Auto MC-Reward thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex: 4; margin-left: 20px;">
      <p><strong>Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft</strong></p>
      <p><em>Hao Li*, Xue Yang*, <strong>Zhaokai Wang*</strong>, Xizhou Zhu, Jie Zhou, Yu Qiao, Xiaogang Wang, Hongsheng Li, Lewei Lu, Jifeng Dai</em></p>
      <p>CVPR 2024</p>
      <p><a href="https://arxiv.org/abs/2312.09238">[Paper]</a> <a href="https://yangxue0827.github.io/auto_mc-reward.html">[Project Page]</a> <a href="https://mp.weixin.qq.com/s/P2yCkUKnqYFJiY9bDtppLQ">[Post]</a></p>
    </div>
  </div>


  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex: 2; max-width: 800px;">
      <img src="pub_imgs/musprod.png" alt="Musprod thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex: 4; margin-left: 20px;">
      <p><strong>Video Background Music Generation: Dataset, Method and Evaluation</strong></p>
      <p><em>Le Zhuo*, <strong>Zhaokai Wang*</strong>, Baisen Wang*, Yue Liao, Chenxi Bao, Stanley Peng, Songhao Han, Aixi Zhang, Fei Fang, Si Liu</em></p>
      <p>ICCV 2023</p>
      <p><a href="https://arxiv.org/abs/2211.11248">[Paper]</a> <a href="https://drive.google.com/drive/folders/1ASY44xqWGZgKkcHhpzWlOhIbUIMe_epQ?usp=sharing">[Demo]</a></p>
    </div>
  </div>



  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex: 2; max-width: 800px;">
      <img src="pub_imgs/cmt.png" alt="CMT thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex: 4; margin-left: 20px;">
      <p><strong>Video Background Music Generation with Controllable Music Transformer</strong></p>
      <p><em>Shangzhe Di, Zeren Jiang, Si Liu, <strong>Zhaokai Wang</strong>, Leyan Zhu, Zexin He, Hongming Liu, Shuicheng Yan</em></p>
      <p>ACM MM 2021 <strong>Best Paper Award (1/542)</strong></p>
      <p><a href="https://arxiv.org/abs/2111.08380">[Paper]</a> <a href="https://wzk1015.github.io/cmt/">[Project Page]</a> <a href="https://github.com/wzk1015/video-bgm-generation">[Code]</a> <a href="https://colab.research.google.com/github/wzk1015/video-bgm-generation/blob/main/CMT.ipynb">[Demo]</a> <a href="https://mp.weixin.qq.com/s/2aFgIq4-zA9tlgGSNuxzWg">[Post]</a> <a href="https://news.buaa.edu.cn/info/1005/54971.htm">[News]</a></p>
    </div>
  </div>



  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex: 2; max-width: 800px;">
      <img src="pub_imgs/cnmt.png" alt="CNMT thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex: 4; margin-left: 20px;">
      <p><strong>Confidence-aware Non-repetitive Multimodal Transformers for TextCaps</strong></p>
      <p><em><strong>Zhaokai Wang</strong>, Renda Bao, Qi Wu, Si Liu</em></p>
      <p>AAAI 2021</p>
      <p><a href="https://arxiv.org/abs/2012.03662">[Paper]</a> <a href="https://github.com/wzk1015/CNMT">[Code]</a></p>
    </div>
  </div>
