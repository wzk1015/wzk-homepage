---
widget: blank

# Activate this widget? true/false
active: true

# This file represents a page section.
headless: false

# Order that this section appears on the page.
weight: 100

title: Publications
subtitle:

design:
  columns: "1"
  background:
    spacing:
      padding: ["50px", "100px", "50px", "100px"]

---

<br>
<br>

<div class="publication-list">
  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex: 2; max-width: 800px;">
      <img src="pub_imgs/mono-internvl.png" alt="Mono-InternVL thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex: 4; margin-left: 20px;">
      <p><strong>Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training</strong></p>
      <p><em>Gen Luo*, Xue Yang*, Wenhan Dou*, <strong>Zhaokai Wang*</strong>, Jifeng Dai, Yu Qiao, Xizhou Zhu</em></p>
      <p>Preprint</p>
      <p><a href="https://arxiv.org/abs/2410.08202">[Paper]</a> <a href="https://internvl.github.io/blog/2024-10-10-Mono-InternVL/">[Project Page]</a> <a href="https://huggingface.co/OpenGVLab/Mono-InternVL-2B">[Code]</a> <a href="https://mp.weixin.qq.com/s/FmjG0Gp5ow7mm2Vzd9ppPg">[Post]</a></p>
    </div>
  </div>
  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex: 2; max-width: 800px;">
      <img src="pub_imgs/sparkle.jpg" alt="Sparkle thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex: 4; margin-left: 20px;">
      <p><strong>Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models Elicits Generalization to Composite Spatial Reasoning</strong></p>
      <p><em>Yihong Tang*, Ao Qu*, <strong>Zhaokai Wang*</strong>, Dingyi Zhuang*, Zhaofeng Wu, Wei Ma, Shenhao Wang, Yunhan Zheng, Zhan Zhao, Jinhua Zhao</em></p>
      <p>Preprint</p>
      <p><a href="https://arxiv.org/abs/2410.16162">[Paper]</a></p>
    </div>
  </div>
  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex: 2; max-width: 800px;">
      <img src="pub_imgs/piip.png" alt="PIIP thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex: 4; margin-left: 20px;">
      <p><strong>Parameter-Inverted Image Pyramid Networks</strong></p>
      <p><em>Xizhou Zhu*, Xue Yang*, <strong>Zhaokai Wang*</strong>, Hao Li, Wenhan Dou, Junqi Ge, Lewei Lu, Yu Qiao, Jifeng Dai</em></p>
      <p>NeurIPS 2024 <strong>(Spotlight)</strong></p>
      <p><a href="https://arxiv.org/abs/2406.04330">[Paper]</a> <a href="https://github.com/OpenGVLab/PIIP">[Code]</a> <a href="https://zhuanlan.zhihu.com/p/705734540">[Post]</a> <a href="https://www.wzk.plus/uploads/slides/PIIP_slides.pdf">[Slides]</a> <a href="https://youtu.be/Kdh3CNp8bfg">[Video]</a></p>
    </div>
  </div>




  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex: 2; max-width: 800px;">
      <img src="pub_imgs/itinera1.jpg" alt="ITINERA thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex: 4; margin-left: 20px;">
      <p><strong>ITINERA: Integrating Spatial Optimization with Large Language Models for Open-domain Urban Itinerary Planning</strong></p>
      <p><em>Yihong Tang*, <strong>Zhaokai Wang*</strong>, Ao Qu*, Yihao Yan*, Zhaofeng Wu, Dingyi Zhuang, Jushi Kai, Kebing Hou, Xiaotong Guo, Jinhua Zhao, Zhan Zhao, Wei Ma</em></p>
      <p>EMNLP 2024</p>
      <p><a href="https://arxiv.org/abs/2402.07204">[Paper]</a> <a href="https://github.com/YihongT/ITINERA">[Code]</a> <a href="https://mp.weixin.qq.com/s/44mtENyqrHiNEEcWS61COg">[Post]</a></p>
    </div>
  </div>

  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex: 2; max-width: 800px;">
      <img src="pub_imgs/itinera2.jpg" alt="Synergizing Spatial Optimization thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex: 4; margin-left: 20px;">
      <p><strong>Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning</strong></p>
      <p><em>Yihong Tang*, <strong>Zhaokai Wang*</strong>, Ao Qu*, Yihao Yan*, Kebing Hou, Dingyi Zhuang, Xiaotong Guo, Jinhua Zhao, Zhan Zhao, Wei Ma</em></p>
      <p>KDD UrbComp 2024 <strong>(Best Paper Award)</strong></p>
      <p><a href="https://arxiv.org/abs/2402.07204">[Paper]</a> <a href="https://github.com/YihongT/ITINERA">[Code]</a> <a href="https://mp.weixin.qq.com/s/44mtENyqrHiNEEcWS61COg">[Post]</a></p>
    </div>
  </div>

  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex: 2; max-width: 800px;">
      <img src="pub_imgs/auto_mc_reward.png" alt="Auto MC-Reward thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex: 4; margin-left: 20px;">
      <p><strong>Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft</strong></p>
      <p><em>Hao Li*, Xue Yang*, <strong>Zhaokai Wang*</strong>, Xizhou Zhu, Jie Zhou, Yu Qiao, Xiaogang Wang, Hongsheng Li, Lewei Lu, Jifeng Dai</em></p>
      <p>CVPR 2024</p>
      <p><a href="https://arxiv.org/abs/2312.09238">[Paper]</a> <a href="https://yangxue0827.github.io/auto_mc-reward.html">[Project Page]</a> <a href="https://mp.weixin.qq.com/s/P2yCkUKnqYFJiY9bDtppLQ">[Post]</a></p>
    </div>
  </div>

  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex: 2; max-width: 800px;">
      <img src="pub_imgs/musprod.png" alt="Video Background Music Generation thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex: 4; margin-left: 20px;">
      <p><strong>Video Background Music Generation: Dataset, Method and Evaluation</strong></p>
      <p><em>Le Zhuo*, <strong>Zhaokai Wang*</strong>, Baisen Wang*, Yue Liao, Chenxi Bao, Stanley Peng, Songhao Han, Aixi Zhang, Fei Fang, Si Liu</em></p>
      <p>ICCV 2023</p>
      <p><a href="https://arxiv.org/abs/2211.11248">[Paper]</a> <a href="https://drive.google.com/drive/folders/1ASY44xqWGZgKkcHhpzWlOhIbUIMe_epQ?usp=sharing">[Demo]</a></p>
    </div>
  </div>

  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex: 2; max-width: 800px;">
      <img src="pub_imgs/cmt.png" alt="Video Background Music Generation with Controllable Music Transformer thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex: 4; margin-left: 20px;">
      <p><strong>Video Background Music Generation with Controllable Music Transformer</strong></p>
      <p><em>Shangzhe Di, Zeren Jiang, Si Liu, <strong>Zhaokai Wang</strong>, Leyan Zhu, Zexin He, Hongming Liu, Shuicheng Yan</em></p>
      <p>ACM MM 2021 <strong>(Best Paper Award)</strong></p>
      <p><a href="https://arxiv.org/abs/2111.08380">[Paper]</a> <a href="https://wzk1015.github.io/cmt/">[Project Page]</a> <a href="https://github.com/wzk1015/video-bgm-generation">[Code]</a> <a href="https://colab.research.google.com/github/wzk1015/video-bgm-generation/blob/main/CMT.ipynb">[Demo]</a> <a href="https://mp.weixin.qq.com/s/2aFgIq4-zA9tlgGSNuxzWg">[Post]</a> <a href="https://news.buaa.edu.cn/info/1005/54971.htm">[News]</a></p>
    </div>
  </div>

  <div class="publication-item" style="display: flex; align-items: center; margin-bottom: 20px;">
    <div class="publication-image" style="flex: 2; max-width: 800px;">
      <img src="pub_imgs/cnmt.png" alt="Confidence-aware Non-repetitive Multimodal Transformers thumbnail" style="width: 100%; height: auto;" />
    </div>
    <div class="publication-text" style="flex: 4; margin-left: 20px;">
      <p><strong>Confidence-aware Non-repetitive Multimodal Transformers for TextCaps</strong></p>
      <p><em><strong>Zhaokai Wang</strong>, Renda Bao, Qi Wu, Si Liu</em></p>
      <p>AAAI 2021</p>
      <p><a href="https://arxiv.org/abs/2012.03662">[Paper]</a> <a href="https://github.com/wzk1015/CNMT">[Code]</a></p>
    </div>
  </div>
